{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyknos\n",
    "!pip install pyro-ppl\n",
    "!pip install zuko\n",
    "!pip install sbi --no-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: The Lotka-volterra simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will perform Simulation-based inference on the Lotka-Volterra simulator.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lotka-Volterra simulator models the population of prey and predator, given four parameters:\n",
    "\n",
    "- $\\alpha$: Prey birth rate: The rate at which prey reproduce in the absence of predators\n",
    "- $\\beta$: Predation rate: The rate at which predators consume prey, reducing the prey population\n",
    "- $\\delta$: Predator reproduction rate: The rate at which predators reproduce based on prey consumption\n",
    "- $\\gamma$: Predator death rate: The rate at which predators die in the absence of prey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "_ = np.random.seed(0)\n",
    "\n",
    "def lotka_volterra(y, alpha, beta, delta, gamma):\n",
    "    prey, predator = y\n",
    "    dprey_dt = alpha * prey - beta * prey * predator\n",
    "    dpredator_dt = delta * prey * predator - gamma * predator\n",
    "    return np.asarray([dprey_dt, dpredator_dt])\n",
    "\n",
    "def simulate(parameters):\n",
    "    alpha = parameters[0]\n",
    "    beta = parameters[1]\n",
    "    delta = parameters[2]\n",
    "    gamma = parameters[3]\n",
    "\n",
    "    y0 = np.asarray([40.0, 9.0])  # Initial populations\n",
    "    t_span = 200  # Total simulation time\n",
    "    dt = 0.1  # Time step\n",
    "\n",
    "    timesteps = int(t_span / dt)\n",
    "    y = np.zeros((timesteps, 2))\n",
    "    y[0] = y0\n",
    "\n",
    "    for i in range(1, timesteps):\n",
    "        y[i] = y[i-1] + lotka_volterra(y[i-1], alpha, beta, delta, gamma) * dt\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect this simulator a set of example parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "time_vec = np.arange(0, 200, 0.1)\n",
    "\n",
    "alpha = 0.1\n",
    "beta = 0.02\n",
    "delta = 0.01\n",
    "gamma = 0.1\n",
    "\n",
    "observation = simulate(np.asarray([alpha, beta, delta, gamma]))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "_ = ax.plot(time_vec, observation)\n",
    "_ = ax.legend([\"Prey\", \"Predator\"])\n",
    "_ = ax.set_xlabel(\"Time\")\n",
    "_ = ax.set_ylabel(\"Population\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Task 1: Explore the simulator. Try running the simulator with different parameters. For example, decrease the birth rate of Prey $\\alpha$ and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parallelize the simulation across many different parameter sets, you can (later on) use the following helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def parallel_simulate(theta):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        theta: A batch of parameters of shape (N, 4).\n",
    "    \n",
    "    Returns:\n",
    "        A batch of time series of shape (N, Time).\n",
    "    \"\"\"\n",
    "    # Our simulator uses numpy, but prior samples are in PyTorch.\n",
    "    theta_np = theta.numpy()\n",
    "\n",
    "    num_workers = 8\n",
    "    simulation_outputs = Parallel(n_jobs=num_workers)(\n",
    "        delayed(simulate)(batch)\n",
    "        for batch in theta_np\n",
    "    )\n",
    "    return np.asarray(simulation_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise and summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we will not be able to observe populations exactly, but only with noise. In addition, in this example, we will aim to reproduce summary statistics of these simulations. Let's add noise and define these statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_simulation(simulation_result):\n",
    "    observation_noise = np.reshape(\n",
    "        np.random.randn(2000 * 2),\n",
    "        (2000, 2)\n",
    "    )\n",
    "    noisy_sim = simulation_result + observation_noise\n",
    "\n",
    "    prey_population = noisy_sim[:, 0]\n",
    "    predator_population = noisy_sim[:, 1]\n",
    "    summary = [\n",
    "        np.max(prey_population).item(),\n",
    "        np.max(predator_population).item(),\n",
    "        np.mean(prey_population).item(),\n",
    "        np.mean(predator_population).item()\n",
    "    ]\n",
    "    return np.asarray(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the observation $x_o$ for which we will aim to obtain the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_o = summarize_simulation(observation)\n",
    "print(x_o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform Bayesian inference, we also have to define a prior over the four parameters. In this case, we assume a uniform prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "_ = torch.manual_seed(42)\n",
    "from sbi.utils import BoxUniform\n",
    "\n",
    "lower_bound = torch.as_tensor([0.05, 0.01, 0.005, 0.005])\n",
    "upper_bound = torch.as_tensor([0.15, 0.03, 0.03, 0.15])\n",
    "prior = BoxUniform(low=lower_bound, high=upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Performing Approximate Bayesian Computation (ABC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now aim to use Approximate Bayesian Computation (ABC) to obtain approximate posterior samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we draw 1000 samples from the prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "theta = prior.sample((N,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for each parameter set, we run the simulation and compute the summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_outputs = parallel_simulate(theta)\n",
    "x = np.asarray([summarize_simulation(sim) for sim in simulation_outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then visualize the simulation outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 3))\n",
    "_ = ax[0].plot(time_vec, observation[:, 0], c=\"k\")\n",
    "_ = ax[0].plot(time_vec, simulation_outputs[::10, :, 0].T, alpha=0.1, c=\"b\")\n",
    "_ = ax[0].set_title(\"Prey\")\n",
    "\n",
    "_ = ax[1].plot(time_vec, observation[:, 1], c=\"k\")\n",
    "_ = ax[1].plot(time_vec, simulation_outputs[::10, :, 1].T, alpha=0.1, c=\"b\")\n",
    "_ = ax[1].set_title(\"Predator\")\n",
    "\n",
    "_ = ax[1].legend([\"Observation\", \"Posterior predictives\"])\n",
    "_ = ax[0].set_xlabel(\"Time\")\n",
    "_ = ax[0].set_ylabel(\"Population\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, most simulation outputs (blue) are vastly different from the observation (black)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for each simulation output $x$, we compute the Euclidean distance to the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `x_o`: Observation (shape (4,))\n",
    "# `x`: Simulation outputs (shape (N, 4))\n",
    "# `distance`: Euclidean distance between and x and x_o (shape (1000,))\n",
    "\n",
    "distance = np.sum((x_o - x)**2, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then select every parameter set `theta` whose simulation output which has a distance closer than `epsilon=10.0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 10.0\n",
    "condition = distance < epsilon\n",
    "theta_abc = theta[condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the posterior distribution with the `pairplot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.analysis import pairplot\n",
    "\n",
    "limits = torch.stack([lower_bound, upper_bound]).T\n",
    "_ = pairplot(theta_abc, limits=limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above `pairplot` shows the 1D marginals (on the diagonals) and the 2D marginals (upper diagonals) of the posterior distribution as histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then visualize **posterior predictive samples**, i.e. simulation outputs given approximate posterior samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_predictives = parallel_simulate(theta_abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9, 3))\n",
    "_ = ax[0].plot(time_vec, observation[:, 0], c=\"k\")\n",
    "_ = ax[0].plot(time_vec, posterior_predictives[:, :, 0].T, alpha=0.3, c=\"b\")\n",
    "_ = ax[0].set_title(\"Prey\")\n",
    "\n",
    "_ = ax[1].plot(time_vec, observation[:, 1], c=\"k\")\n",
    "_ = ax[1].plot(time_vec, posterior_predictives[:, :, 1].T, alpha=0.3, c=\"b\")\n",
    "_ = ax[1].set_title(\"Predator\")\n",
    "\n",
    "_ = ax[1].legend([\"Observation\", \"Posterior predictives\"])\n",
    "_ = ax[0].set_xlabel(\"Time\")\n",
    "_ = ax[0].set_ylabel(\"Population\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the simulation outputs are now closer to the observation.\n",
    "\n",
    "You just performed Approximate Bayesian Computation (ABC). A core limitation of ABC is that it requires to tune a threshold `epsilon` (above: 10.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Play around with different thresholds and see what happens. What problems does this cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Running Neural Posterior Estimation (NPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Posterior Estimation (NPE) avoids having to define an `epsilon`. NPE trains a neural network to directly predict the posterior distribution for any `x_o`. Below, you will use the `sbi` toolbox to perform NPE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating training data\n",
    "\n",
    "NPE begins just like ABC: by drawing samples from the prior and running the simulator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we generate 1000 prior samples, run the simulation, and compute summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(42)\n",
    "\n",
    "N = 1000\n",
    "theta = prior.sample((N,))\n",
    "\n",
    "simulation_outputs = parallel_simulate(theta)\n",
    "x = np.asarray([summarize_simulation(sim) for sim in simulation_outputs])\n",
    "x = torch.as_tensor(x, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use NPE with a neural spline flow (`nsf`) as density estimator. If you are interested, see [here](https://sbi.readthedocs.io/en/latest/sbi.html#neural-nets) for the collection of neural networks and [here](https://sbi.readthedocs.io/en/latest/how_to_guide/03_choose_neural_net.html) for a guide on which methods to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.inference import NPE\n",
    "\n",
    "inference = NPE(density_estimator=\"nsf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train the network on our simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_net = inference.append_simulations(theta, x).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the convergence of the training loop by inspecting its loss curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.analysis import plot_summary\n",
    "\n",
    "_ = plot_summary(\n",
    "    inference,\n",
    "    tags=[\"training_loss\", \"validation_loss\"],\n",
    "    figsize=(10, 2),\n",
    ")\n",
    "# All training logs are available in `trainer.summary`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the validation loss seems to still be going down, so it might be worth setting a higher value for `.train(stop_after_epochs=30)` (default value is 20). This parameter sets how many epochs must pass for without the validation loss reaching a new minimum. For this example, we will not retrain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can build the `posterior`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = inference.build_posterior()\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring the posterior\n",
    "\n",
    "Let's aim to infer the posterior given our observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Observation: \", x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We draw samples from the NPE-posterior by drawing samples from the trained conditional generative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = posterior.sample((1_000,), x=x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can infer the posterior distribution for any observation $x_{o}$ without having to run new simulations and without having to re-train. This property is called **amortization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize these posterior samples with the `pairplot` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.analysis import pairplot\n",
    "\n",
    "_ = pairplot(\n",
    "    samples,\n",
    "    limits=limits,\n",
    "    ticks=limits,\n",
    "    figsize=(5, 5),\n",
    "    labels=[r\"$\\alpha$\", r\"$\\beta$\", r\"$\\delta$\", r\"$\\gamma$\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify potential issues in the posterior estimate, we then perform **posterior predictive checks**. As above for ABC, simulate posterior samples and compare them to the observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = posterior.sample((10,), x=x_o)\n",
    "posterior_predictives = parallel_simulate(posterior_samples)\n",
    "\n",
    "posterior_predictive_summary_stats = torch.as_tensor(\n",
    "    np.asarray([summarize_simulation(sim) for sim in simulation_outputs]),\n",
    "    dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9, 3))\n",
    "_ = ax[0].plot(time_vec, observation[:, 0], c=\"k\")\n",
    "_ = ax[0].plot(time_vec, posterior_predictives[:, :, 0].T, alpha=0.3, c=\"b\")\n",
    "_ = ax[0].set_title(\"Prey\")\n",
    "\n",
    "_ = ax[1].plot(time_vec, observation[:, 1], c=\"k\")\n",
    "_ = ax[1].plot(time_vec, posterior_predictives[:, :, 1].T, alpha=0.3, c=\"b\")\n",
    "_ = ax[1].set_title(\"Predator\")\n",
    "\n",
    "_ = ax[1].legend([\"Observation\", \"Posterior predictives\"])\n",
    "_ = ax[0].set_xlabel(\"Time\")\n",
    "_ = ax[0].set_ylabel(\"Population\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the posterior predictives (light blue and light orange) look similar to the observation (blue and orange). They do not look exactly the same, but this is expected: we only used the maximum and mean of the two traces as summary statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: embedding networks, other methods, and model misspecification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section. You can explore making the inference above more accurate. Here are a few ideas:\n",
    "\n",
    "> 1) Run more simulations. Re-run training and inference. What do you observe?  \n",
    "\n",
    "> 2) Do not extract summary statistics, but perform inference on the whole time series. Then, [define a neural networks with an embedding network](https://sbi.readthedocs.io/en/latest/how_to_guide/04_embedding_networks.html). Typically, you will need more simulations to train an embedding network.  \n",
    "\n",
    "> 3) You can also try out other methods. For example, replace `NPE` above with `NLE` or `NRE` (but note that, for this task, we do not expect a major improvement in performance for these methods.  \n",
    "\n",
    "> 4) Alternatively, you can also check out other simulators. For example, you can check out [this tutorial for a Hodgkin-HJuxley neuroscience simulator](https://sbi.readthedocs.io/en/latest/tutorials/Example_00_HodgkinHuxleyModel.html) or [this tutorial for a drift-diffusion model from psychophysics](https://sbi.readthedocs.io/en/latest/tutorials/Example_01_DecisionMakingModel.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Diagnosing potential issues in the posterior\n",
    "\n",
    "How do we know that the posterior is correct? The `sbi` toolbox implements a wide range\n",
    "of methods that diagnose potential issues (more detail in [this how-to guide](https://sbi.readthedocs.io/en/latest/how_to_guide/14_choose_diagnostic_tool.html)). Below, we will perform _simulation-based calibration_ (SBC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation-based calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posterior-predictive checks are an ad-hoc heuristic for assessing posterior quality. Simulation-based calibration (SBC) provides a _quantitative_ assessment of posterior quality. It allows you to check, for every parameter, whether the parameter is estimated as (on average) to low, to high, and whether it has too low or two high uncertainty. To run SBC, you first have to generate more simulations based on prior samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sbc_samples = 200  # choose a number of sbc runs, should be ~100s\n",
    "\n",
    "prior_samples = prior.sample((num_sbc_samples,))\n",
    "\n",
    "prior_predictives = parallel_simulate(prior_samples)\n",
    "prior_predictive_summary_stats = torch.as_tensor(\n",
    "    np.asarray([summarize_simulation(sim) for sim in prior_predictives]),\n",
    "    dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SBC is implemented in `sbi` for your use on any `sbi` posterior. To run it, we only need to call `run_sbc` with appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.diagnostics import run_sbc\n",
    "\n",
    "# run SBC: for each inference we draw 1000 posterior samples.\n",
    "num_posterior_samples = 1_000\n",
    "ranks, dap_samples = run_sbc(\n",
    "    prior_samples,\n",
    "    prior_predictive_summary_stats,\n",
    "    posterior,\n",
    "    reduce_fns=lambda theta, x: -posterior.log_prob(theta, x),\n",
    "    num_posterior_samples=num_posterior_samples,\n",
    "    use_batched_sampling=False,  # `True` can give speed-ups, but can cause memory issues.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For amortized neural posteriors (like in this tutorial), execution of `sbc` is expected to be fast. For posteriors that conduct inference with MCMC and hence are slow, `run_sbc` exposes the use of multiple internal parallel workers to the user. To use this feature, add `num_workers = 2` to the parameters for use of two workers. See the API documentation for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the posterior approximation is faithful, the `ranks` should be uniformly distributed. We can evaluate this with by plotting a histogram of the resulting ranks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.analysis.plot import sbc_rank_plot\n",
    "\n",
    "fig, ax = sbc_rank_plot(\n",
    "    ranks,\n",
    "    num_posterior_samples,\n",
    "    plot_type=\"cdf\",\n",
    "    num_bins=20,\n",
    "    figsize=(5, 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gray are marks the region which is still sufficiently uniform (meaning that we cannot reject the null hypothesis that the samples are uniform). In this case, all ranks (in red) are within this gray band. This is a good sign, but it is only a necessary---but not a sufficient---condition for the posterior to be correct. To this end, `sbi` implements a wide range of diagnostic methods, which are described [here](https://sbi.readthedocs.io/en/latest/how_to_guide/14_choose_diagnostic_tool.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
